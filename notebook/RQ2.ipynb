{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Questions 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from nltk import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "from array import array\n",
    "from heapq import heappush, heappushpop, heappop, heapify\n",
    "from gensim.models import Word2Vec\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tweet:\n",
    "    def __init__(self, id, text, user, date, url,\n",
    "                 hashtags, likes, retweets, terms):\n",
    "        self.id = id\n",
    "        self.text = text\n",
    "        self.user = user\n",
    "        self.date = date\n",
    "        self.url = url\n",
    "        self.hashtags = hashtags\n",
    "        self.likes = likes\n",
    "        self.retweets = retweets\n",
    "        self.terms = terms\n",
    "\n",
    "    def get_terms(self):\n",
    "        return self.terms\n",
    "\n",
    "    @staticmethod\n",
    "    def from_json(json_str):\n",
    "        tweet_json = json.loads(json_str)\n",
    "        return Tweet(\n",
    "            tweet_json['ID'], tweet_json['Tweet_text'], tweet_json['UserId'],\n",
    "            tweet_json['Date'], tweet_json['URL'], tweet_json['Hashtags'],\n",
    "            tweet_json['Likes'], tweet_json['Number_Retweets'], tweet_json['terms']\n",
    "        )\n",
    "\n",
    "    def to_json(self):\n",
    "        return {\n",
    "            'ID': self.id,\n",
    "            'Tweet_text': self.text,\n",
    "            'UserId': self.user,\n",
    "            'Date': self.date,\n",
    "            'URL': self.url,\n",
    "            'Hashtags': self.hashtags,\n",
    "            'Likes': self.likes,\n",
    "            'Number_Retweets': self.retweets,\n",
    "            'terms': self.terms\n",
    "        }\n",
    "\n",
    "    def tf_idf_score(self, index):\n",
    "        tweet_v = []\n",
    "        tweet_tf = index.get_tf(self, self.id)\n",
    "        idfs = index.get_idf(self)\n",
    "        for i in range(len(query_tf)):\n",
    "            tweet_v.append(tweet_tf[i] * idfs[i])\n",
    "            query_v.append(query_tf[i] * idfs[i])\n",
    "\n",
    "    def __str__(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_terms(line):\n",
    "    \"\"\"\n",
    "    Preprocess the article text (title + body) removing stop words, stemming,\n",
    "    transforming in lowercase and return the tokens of the text.\n",
    "\n",
    "    Argument:\n",
    "    line -- string (text) to be preprocessed\n",
    "\n",
    "    Returns:\n",
    "    line - a list of tokens corresponding to the input text after the preprocessing\n",
    "    \"\"\"\n",
    "\n",
    "    stemming = PorterStemmer()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    ## START CODE\n",
    "    line = line.lower()  ## Transform in lowercase\n",
    "    line = re.sub(r'http\\S+', '', line)\n",
    "    line = re.sub(r'&amp', '', line)\n",
    "    line = re.sub(r'\\d+', '', line)\n",
    "    line = re.sub(r'\\W', ' ', line)\n",
    "    line = line.split()  ## Tokenize the text to get a list of terms\n",
    "    line = [word for word in line if word not in stops]  ##eliminate the stopwords (HINT: use List Comprehension)\n",
    "    line = [stemming.stem(word)\n",
    "            for word in line]  ## perform stemming (HINT: use List Comprehension)\n",
    "    ## END CODE\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Query:\n",
    "    def __init__(self, query_text):\n",
    "        self.text = query_text\n",
    "        self.terms = get_terms(self.text)\n",
    "        self.tf = {}\n",
    "        for term in self.terms:\n",
    "            if term not in self.tf:\n",
    "                self.tf[term] = 0\n",
    "            self.tf[term] += 1\n",
    "\n",
    "    def get_terms(self):\n",
    "        return list(self.tf.keys())\n",
    "\n",
    "    def get_tf(self):\n",
    "        return list(self.tf.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Index:\n",
    "\n",
    "    def __init__(self, repr_size=128):\n",
    "        self.index= None # term --> tweet\n",
    "        self.tf = None\n",
    "        self.df = None\n",
    "        self.tweets = {}\n",
    "        self.max_fav = 0\n",
    "        self.max_rt = 0\n",
    "        self.repr_size = repr_size\n",
    "        self.topidf = None\n",
    "\n",
    "    def load_json_tweets(self, file):\n",
    "        with open(file, 'r') as fp:\n",
    "            self.create_index(fp.readlines())\n",
    "\n",
    "    def save(self, path):\n",
    "        with open(path, 'wb') as fp:\n",
    "            pickle.dump(self, fp)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path):\n",
    "        with open(path, 'rb') as fp:\n",
    "            index = pickle.load(fp)\n",
    "        return index\n",
    "\n",
    "    def create_index(self, lines):\n",
    "        \"\"\"\n",
    "        Implement the inverted index\n",
    "        \n",
    "        Argument:\n",
    "        lines -- collection of tweets\n",
    "        \n",
    "        Returns:\n",
    "        index - the inverted index (implemented through a python dictionary) containing terms as keys and the corresponding \n",
    "        list of tweets these keys appears in (and the positions) as values.\n",
    "        \"\"\"\n",
    "\n",
    "        index=defaultdict(list)\n",
    "        tf = defaultdict(dict)\n",
    "        df = defaultdict(int)\n",
    "\n",
    "        for line in lines: # Remember, lines contain all tweets, each line is a tweet\n",
    "            tweet_json = json.loads(line)\n",
    "            tweet = Tweet(\n",
    "                int(tweet_json['ID']), tweet_json['Tweet_text'], tweet_json['UserId'],\n",
    "                tweet_json['Date'], tweet_json['URL'], tweet_json['Hashtags'],\n",
    "                tweet_json['Likes'], tweet_json['Number_Retweets'], tweet_json['terms']\n",
    "            )\n",
    "            if tweet.id in self.tweets:\n",
    "                continue\n",
    "            self.tweets[tweet.id] = tweet\n",
    "            tweet_id = tweet.id #tweet id\n",
    "            terms = tweet.terms #page_title + page_text\n",
    "            self.max_fav = max(self.max_fav, tweet.likes)\n",
    "            self.max_rt = max(self.max_rt, tweet.retweets)\n",
    "            #title = line_arr[1]            \n",
    "            #titleIndex[page_id]=title  ## we do not need to apply get terms to title because it used only to print titles and not in the index\n",
    "            \n",
    "            ## ===============================================================        \n",
    "            ## create the index for the current doc and store it in termdictPage\n",
    "            ## termdictPage ==> { ‘term1’: [currentdoc, [list of positions]], ...,‘termn’: [currentdoc, [list of positions]]}\n",
    "            \n",
    "            ## Example: if the curr_doc has id 1 and his text is \n",
    "            ## \"web retrieval information retrieval\":\n",
    "            \n",
    "            ## termdictPage ==> { ‘web’: [1, [0]], ‘retrieval’: [1, [1,3]], ‘information’: [1, [2]]}\n",
    "            \n",
    "            ## the term ‘web’ appears in document 1 in positions 0, \n",
    "            ## the term ‘retrieval’ appears in document 1 in positions 1 and 3\n",
    "            ## ===============================================================\n",
    "            \n",
    "            termdictTweets={}\n",
    "\n",
    "            for position, term in enumerate(terms): # terms contains page_title + page_text. Loop over all terms\n",
    "                try:\n",
    "                    # if the term is already in the index for the current page (termdictPage)\n",
    "                    # append the position to the corrisponding list\n",
    "                    \n",
    "            ## START CODE\n",
    "                    termdictTweets[term][1].append(position)  \n",
    "                except:\n",
    "                    # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                    termdictTweets[term]=[tweet_id, array('I',[position])] #'I' indicates unsigned int (int in python)\n",
    "                \n",
    "            #merge the current page index with the main index\n",
    "            for termpage, postingpage in termdictTweets.items():\n",
    "                index[termpage].append(postingpage)\n",
    "\n",
    "            # normalize term frequencies\n",
    "            # Compute the denominator to normalize term frequencies (formula 2 above)\n",
    "            # norm is the same for all terms of a document.\n",
    "            norm = 0\n",
    "            for term, posting in termdictTweets.items():\n",
    "                # posting is a list containing doc_id and the list of positions for current term in current document:\n",
    "                # posting ==> [currentdoc, [list of positions]]\n",
    "                # you can use it to inferr the frequency of current term.\n",
    "                norm += len(posting[1]) ** 2\n",
    "            norm = math.sqrt(norm)\n",
    "\n",
    "            # calculate the tf(dividing the term frequency by the above computed norm) and df weights\n",
    "            for term, posting in termdictTweets.items():\n",
    "                # append the tf for current term (tf = term frequency in current doc/norm)\n",
    "                tf[term][posting[0]] = np.round(len(posting[1]) / norm, 4) ## SEE formula (1) above\n",
    "                # increment the document frequency of current term (number of documents containing the current term)\n",
    "                df[term] += 1  # increment df for current term\n",
    "            ## END CODE                    \n",
    "        self.index = index\n",
    "        self.tf = tf\n",
    "        self.df = df\n",
    "        N = len(self.tweets)\n",
    "        self.topidf = []\n",
    "        for term, df in self.df.items():\n",
    "            if len(self.topidf) < self.repr_size:\n",
    "                heappush(self.topidf, (df, term))\n",
    "            else:\n",
    "                heappushpop(self.topidf, (df, term))\n",
    "\n",
    "    #Given a query (set of words) and and index return the idf of that query\n",
    "    def get_idf(self, query):\n",
    "        N=len(self.tweets)\n",
    "        idf=[]\n",
    "        query=query.get_terms()\n",
    "        for term in query:\n",
    "            df=self.df[term]\n",
    "            idf.append(math.log(N/df))\n",
    "        return idf\n",
    "\n",
    "    #Given a query (set of words) and and index return the total tf of that query \n",
    "    def get_tf(self, query, tweet_id):\n",
    "        query=query.get_terms()\n",
    "        global_tf_list=[]\n",
    "        term_list=[]\n",
    "        for term in query:\n",
    "            term_list.append(self.tf[term][tweet_id])\n",
    "        return term_list\n",
    "\n",
    "    def get_tf_idf_vector(self, tweet_id):\n",
    "        vector = np.zeros(self.repr_size)\n",
    "        N = len(self.tweets)\n",
    "        total = 0\n",
    "        for i, (df, term) in enumerate(self.topidf):\n",
    "            tf = self.tf[term].get(tweet_id, 0)\n",
    "            tf_idf = tf * math.log(N/df)\n",
    "            vector[i] = tf_idf\n",
    "        norm = np.linalg.norm(vector)\n",
    "        if norm == 0:\n",
    "            return vector\n",
    "        return vector / norm\n",
    "\n",
    "    def get_all_tf_idf(self):\n",
    "        vectors = np.array([])\n",
    "        for tweet in self.tweets:\n",
    "            v = self.get_tf_idf_vector(tweet)\n",
    "            if isinstance(v, np.ndarray):\n",
    "                vectors = np.vstack((vectors, v))\n",
    "            if len(vectors) % 1000 == 0:\n",
    "                print(len(vectors))\n",
    "        return vectors\n",
    "\n",
    "    #Given a query (set of words) return all the tweets containing the query\n",
    "    def get_tweets(self, query):\n",
    "        query=query.get_terms()\n",
    "        list_tweets=set()\n",
    "        for term in query:\n",
    "            ## START DODE\n",
    "            try:\n",
    "                # store in termDocs the ids of the docs that contain \"term\"\n",
    "                termTweets=[posting[0] for posting in self.index[term]]\n",
    "                # docs = docs Union termDocs\n",
    "                if len(list_tweets) == 0:\n",
    "                    list_tweets = list_tweets.union(termTweets)\n",
    "                else:\n",
    "                    list_tweets = list_tweets.intersection(termTweets)\n",
    "            except:\n",
    "                #term is not in index\n",
    "                pass\n",
    "        list_tweets=list(list_tweets)\n",
    "\n",
    "        tweets_query = [self.tweets[tid] for tid in list_tweets] #asssuming index of tweets is id\n",
    "        #list of tweets ids\n",
    "        return tweets_query\n",
    "\n",
    "    #Given a query (set of words) return the tweets number of tweets containing the query\n",
    "    def get_num_tweets(self,query):\n",
    "\n",
    "        num_tweets=len(self.get_tweets(query))\n",
    "\n",
    "        return num_tweets\n",
    "\n",
    "    def get_max_fav(self):\n",
    "        return self.max_fav\n",
    "\n",
    "    def get_max_rt(self):\n",
    "        return self.max_rt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b, norm_a=True, norm_b=True):\n",
    "    total = 0\n",
    "    sum_a = 0 if norm_a else 1.0\n",
    "    sum_b = 0 if norm_b else 1.0\n",
    "    for x, y in zip(a, b):\n",
    "        total += x * y\n",
    "        if norm_a:\n",
    "            sum_a += x * x\n",
    "        if norm_b:\n",
    "            sum_b += y * y\n",
    "    if norm_a:\n",
    "        sum_a = math.sqrt(sum_a)\n",
    "    if norm_b:\n",
    "        sum_b = math.sqrt(sum_b)\n",
    "    return total / (sum_a * sum_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TfIdfScorer:\n",
    "    def __init__(self, index):\n",
    "        self.index = index\n",
    "\n",
    "    def get_tweet_vector(self, tweet):\n",
    "        return self.index.get_tf_idf_vector(tweet.id)\n",
    "\n",
    "    def score(self, tweet: Tweet, query, normalize=True):\n",
    "        tweet_v = []\n",
    "        query_v = []\n",
    "        tweet_tf = self.index.get_tf(query, tweet.id)\n",
    "        query_tf = query.get_tf()\n",
    "        idfs = self.index.get_idf(query)\n",
    "        for i in range(len(query_tf)):\n",
    "            tweet_v.append(tweet_tf[i] * idfs[i])\n",
    "            query_v.append(query_tf[i] * idfs[i])\n",
    "        return cosine_similarity(tweet_v, query_v, norm_a=normalize, norm_b=normalize) # assume get_tf normalizes\n",
    "\n",
    "class CustomScorer:\n",
    "    W_TFIDF = .5\n",
    "    W_FAV = .25\n",
    "    W_RT = .25\n",
    "    def __init__(self, index):\n",
    "        self.index = index\n",
    "\n",
    "    def get_tweet_vector(self, tweet):\n",
    "        return self.index.get_tf_idf_vector(tweet.id)\n",
    "\n",
    "    def score(self, tweet: Tweet, query, normalize=True):\n",
    "        tfidf = TfIdfScorer(self.index).score(tweet, query, normalize=normalize)\n",
    "        max_fav = self.index.get_max_fav()\n",
    "        max_rt = self.index.get_max_rt()\n",
    "        fav_score = math.log(tweet.likes + 1)/(math.log(max_fav + 1) + 1)# TQM\n",
    "        rt_score = math.log(tweet.retweets + 1)/(math.log(max_rt + 1) + 1)\n",
    "        score = tfidf * CustomScorer.W_TFIDF + \\\n",
    "                    fav_score * CustomScorer.W_FAV + \\\n",
    "                    rt_score * CustomScorer.W_RT\n",
    "        return score\n",
    "\n",
    "class Word2VecScorer:\n",
    "    def __init__(self, tweets=[], size=100, window=5, min_count=1, epochs=5):\n",
    "        sentences = [tweet.get_terms() for tweet in tweets]\n",
    "        self.model = Word2Vec(sentences, size=size, window=window, min_count=min_count, iter=epochs)\n",
    "\n",
    "    def save(self, path):\n",
    "        self.model.save(path)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(self, path):\n",
    "        word2vec = Word2VecScorer()\n",
    "        word2vec.model = Word2Vec.load(path)\n",
    "        return word2vec\n",
    "\n",
    "    def get_tweet_vector(self, tweet):\n",
    "        if len(tweet.get_terms()) == 0:\n",
    "            return np.zeros(self.model.vector_size)\n",
    "        return np.mean([self.get_vector(token) for token in tweet.get_terms()], axis=0)\n",
    "\n",
    "    def get_vector(self, word):\n",
    "        if word not in self.model.wv.vocab:\n",
    "            return np.random.uniform(size=len(self.model.vector_size))\n",
    "        return self.model.wv[word]\n",
    "\n",
    "    def score(self, tweet, query: Query):\n",
    "        tweet_v = np.mean([self.get_vector(token) for token in tweet.get_terms()], axis=0)\n",
    "        query_v = np.mean([self.get_vector(token) for token in query.get_terms()], axis=0)\n",
    "        return cosine_similarity(tweet_v, query_v)\n",
    "\n",
    "class DiversityScore:\n",
    "    W_SCORE = 0.8\n",
    "    W_DIV = 0.2\n",
    "    def __init__(self, base_score):\n",
    "        self.base_score = base_score\n",
    "\n",
    "    def get_diversity(self, tweet, index, ranking):\n",
    "        is_in = tweet.id in {id for _, id in ranking}\n",
    "        count = (len(ranking) - (1 if is_in else 0))\n",
    "        if count == 0:\n",
    "            return 0\n",
    "        v1 = index.get_tf_idf_vector(tweet.id)\n",
    "        total = 0\n",
    "        for _, t in ranking:\n",
    "            total += cosine_similarity(v1, index.get_tf_idf_vector(t))\n",
    "        return 1 - (total / count) # if tweet is in ranking, we take mean over len(ranking)-1, otherwise, over all len(ranking)\n",
    "\n",
    "    def score(self, tweet, query: Query, index, ranking):\n",
    "        score = self.base_score.score(tweet, query, index)\n",
    "        diversity = self.get_diversity(tweet, index, ranking)\n",
    "        return score * DiversityScore.W_SCORE + diversity * DiversityScore.W_DIV\n",
    "\n",
    "def rank_tweets(query: Query, index, K=20, scorer=None, log=False):\n",
    "    scorer = scorer if scorer is not None else TfIdfScorer(index)\n",
    "    heap = []\n",
    "    for tweet in index.get_tweets(query):\n",
    "        score = scorer.score(tweet, query)\n",
    "        if len(heap) < K:\n",
    "            heappush(heap, (score, tweet.id))\n",
    "        else:\n",
    "            heappushpop(heap, (score, tweet.id))\n",
    "    if log:\n",
    "        heap2 = list(heap)\n",
    "        ranking_scores = [heappop(heap2) for _ in range(len(heap2))]\n",
    "        ranking_scores.reverse()\n",
    "        for score, tweet in ranking_scores:\n",
    "            print(score, index.tweets[tweet].text)\n",
    "    ranking = [heappop(heap)[1] for _ in range(len(heap))]\n",
    "    ranking.reverse()\n",
    "\n",
    "    return [index.tweets[tid] for tid in ranking]\n",
    "\n",
    "\n",
    "def rank_tweets_diversity(query: Query, index, K=20, scorer=None, log=False):\n",
    "    scorer = scorer if scorer is not None else TfIdfScorer(index)\n",
    "    heap = DiversityHeap(scorer, query)\n",
    "    for i, tweet in enumerate(index.get_tweets(query)):\n",
    "        if len(heap) < K:\n",
    "            heap.push(tweet)\n",
    "        else:\n",
    "            heap.pushpop(tweet)\n",
    "    if log:\n",
    "        heap2 = list(heap)\n",
    "        ranking_scores = [heappop(heap2) for _ in range(len(heap2))]\n",
    "        ranking_scores.reverse()\n",
    "        for score, tweet in ranking_scores:\n",
    "            print(score, index.tweets[tweet].text)\n",
    "    ranking = [heap.pop() for _ in range(len(heap))]\n",
    "    ranking.reverse()\n",
    "\n",
    "    return [index.tweets[tid] for tid in ranking]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DiversityHeap:\n",
    "    def __init__(self, scorer, query):\n",
    "        self.query = query\n",
    "        self.scorer = scorer\n",
    "        self.ranking = []   #heap containing ranking\n",
    "        self.div_rank = []\n",
    "        self.vectors = {}   #vectors of tweets in ranking\n",
    "        self.scores = {}    #scores of tweets given scorer\n",
    "        self.div = {}       # diversity of tweets wrt other tweets in ranking\n",
    "        self.total_div = {} #total diversity of tweets wrt others in ranking\n",
    "\n",
    "    def push(self, tweet: Tweet):\n",
    "        tid = tweet.id\n",
    "        vector = self.scorer.get_tweet_vector(tweet)\n",
    "        self.vectors[tid] = vector\n",
    "        score = self.scorer.score(tweet, self.query)\n",
    "        self.scores[tid] = score\n",
    "        total = 0\n",
    "        div = {}\n",
    "        for t, tdiv in self.div.items():\n",
    "            d = cosine_similarity(vector, self.vectors[t])\n",
    "            div[t] = d\n",
    "            tdiv[tid] = d\n",
    "            total += d\n",
    "        self.div[tid] = div\n",
    "        self.total_div[tid] = total\n",
    "        heappush(self.ranking, (score, tid))\n",
    "\n",
    "    def pop(self):\n",
    "        to_pop = min((self.scores[t]+self.total_div[t]/len(self), t) for t in self.scores)[1]\n",
    "        div = self.div[to_pop]\n",
    "        self.total_div.pop(to_pop)\n",
    "        for t in self.total_div:\n",
    "            self.total_div[t] -= div[t]\n",
    "        self.scores.pop(to_pop)\n",
    "        self.div.pop(to_pop)\n",
    "        self.vectors.pop(to_pop)\n",
    "        self.ranking = [(score, t) for t, score in self.scores.items()]\n",
    "        heapify(self.ranking)\n",
    "        return to_pop\n",
    "\n",
    "    def pushpop(self, tweet: Tweet):\n",
    "        self.push(tweet)\n",
    "        return self.pop()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.scores)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.ranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load index\n",
    "index = Index()\n",
    "index.load_json_tweets('../res/merge_tweets_wusers.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataframe with the tweets data\n",
    "dictionaries = []\n",
    "with open ('../res/merge_tweets_wusers.json','r') as file:\n",
    "    for line in file: \n",
    "        tweet=json.loads(line)\n",
    "        dictionaries.append(tweet)\n",
    "df = pd.DataFrame(dictionaries)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweet_text</th>\n",
       "      <th>UserId</th>\n",
       "      <th>Date</th>\n",
       "      <th>URL</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Number_Retweets</th>\n",
       "      <th>terms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1335163929757151232</td>\n",
       "      <td>Create a pandemic BEFORE presidential election...</td>\n",
       "      <td>1257238248</td>\n",
       "      <td>Sat Dec 05 10:07:41 +0000 2020</td>\n",
       "      <td>https://twitter.com/RealRichardBail/status/133...</td>\n",
       "      <td>[]</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[creat, pandem, presidenti, elect, thu, justif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1335164973396455425</td>\n",
       "      <td>It's quite a big deal that a central religious...</td>\n",
       "      <td>759348635205181440</td>\n",
       "      <td>Sat Dec 05 10:11:50 +0000 2020</td>\n",
       "      <td>https://twitter.com/TheKafkaDude/status/133516...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[quit, big, deal, central, religi, polit, bodi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1334864856969859075</td>\n",
       "      <td>Rest In Peace @CharlestonPDWV PO Cassie Johnso...</td>\n",
       "      <td>772529887</td>\n",
       "      <td>Fri Dec 04 14:19:17 +0000 2020</td>\n",
       "      <td>https://twitter.com/bigricanman/status/1334864...</td>\n",
       "      <td>[BlueLivesMatter, BacktheBlue]</td>\n",
       "      <td>2402</td>\n",
       "      <td>949</td>\n",
       "      <td>[rest, peac, charlestonpdwv, po, cassi, johnso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1335164982158364673</td>\n",
       "      <td>@CeeTalking @NMudoliwa Actually I started my o...</td>\n",
       "      <td>275046515</td>\n",
       "      <td>Sat Dec 05 10:11:52 +0000 2020</td>\n",
       "      <td>https://twitter.com/lunchout2/status/133516498...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[ceetalk, nmudoliwa, actual, start, busi, 1987...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1335164991952080896</td>\n",
       "      <td>@ElysianaValenti @CupKatie00 @emilybernay @Rud...</td>\n",
       "      <td>2728145463</td>\n",
       "      <td>Sat Dec 05 10:11:54 +0000 2020</td>\n",
       "      <td>https://twitter.com/joeyp0tato1/status/1335164...</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[elysianavalenti, cupkatie00, emilybernay, rud...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ID                                         Tweet_text  \\\n",
       "0  1335163929757151232  Create a pandemic BEFORE presidential election...   \n",
       "1  1335164973396455425  It's quite a big deal that a central religious...   \n",
       "2  1334864856969859075  Rest In Peace @CharlestonPDWV PO Cassie Johnso...   \n",
       "3  1335164982158364673  @CeeTalking @NMudoliwa Actually I started my o...   \n",
       "4  1335164991952080896  @ElysianaValenti @CupKatie00 @emilybernay @Rud...   \n",
       "\n",
       "               UserId                            Date  \\\n",
       "0          1257238248  Sat Dec 05 10:07:41 +0000 2020   \n",
       "1  759348635205181440  Sat Dec 05 10:11:50 +0000 2020   \n",
       "2           772529887  Fri Dec 04 14:19:17 +0000 2020   \n",
       "3           275046515  Sat Dec 05 10:11:52 +0000 2020   \n",
       "4          2728145463  Sat Dec 05 10:11:54 +0000 2020   \n",
       "\n",
       "                                                 URL  \\\n",
       "0  https://twitter.com/RealRichardBail/status/133...   \n",
       "1  https://twitter.com/TheKafkaDude/status/133516...   \n",
       "2  https://twitter.com/bigricanman/status/1334864...   \n",
       "3  https://twitter.com/lunchout2/status/133516498...   \n",
       "4  https://twitter.com/joeyp0tato1/status/1335164...   \n",
       "\n",
       "                         Hashtags  Likes  Number_Retweets  \\\n",
       "0                              []      4                4   \n",
       "1                              []      0                0   \n",
       "2  [BlueLivesMatter, BacktheBlue]   2402              949   \n",
       "3                              []      0                0   \n",
       "4                              []      0                0   \n",
       "\n",
       "                                               terms  \n",
       "0  [creat, pandem, presidenti, elect, thu, justif...  \n",
       "1  [quit, big, deal, central, religi, polit, bodi...  \n",
       "2  [rest, peac, charlestonpdwv, po, cassi, johnso...  \n",
       "3  [ceetalk, nmudoliwa, actual, start, busi, 1987...  \n",
       "4  [elysianavalenti, cupkatie00, emilybernay, rud...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We used word2vec representation, in this case tweet2vec to fit clustering.\n",
    "word2vec = Word2VecScorer(tweets=list(index.tweets.values()), size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1st Qt - e\n",
    "tweets= index.tweets\n",
    "X = [np.array([word2vec.get_tweet_vector(tweet)]) for tweet in tweets.values()]\n",
    "X = np.concatenate(X, axis=0) #This are tweets word2vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "import numpy as np\n",
    "\n",
    "#We use KMeans algorithmn to find the clusters. We tried with other clustering algorithnms but with the size\n",
    "#of our data this one worked the bes. \n",
    "clustering = KMeans(n_clusters=4).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We add a column in our dataset with the cluster label. \n",
    "df['cluster'] = clustering.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rank Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = Query('black people')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_normal = rank_tweets(query, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_diversity = rank_tweets_diversity(query, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.3984962406015037, pvalue=0.08180541617403764)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_normal = [t.id for t in ranking_normal]\n",
    "tweets_diversity = [t.id for t in ranking_diversity]\n",
    "\n",
    "scipy.stats.spearmanr(tweets_normal, tweets_diversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters in normal ranking: 0, 1, 3\n"
     ]
    }
   ],
   "source": [
    "ranking_clusters = set()\n",
    "for tweet in ranking_normal:\n",
    "    cluster = df[df['ID'] == tweet.id]['cluster']\n",
    "    ranking_clusters.add(cluster.values[0])\n",
    "print('Clusters in normal ranking:', ', '.join(map(str,ranking_clusters)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters in diversified ranking: 0, 1, 2, 3\n"
     ]
    }
   ],
   "source": [
    "ranking_clusters = set()\n",
    "for tweet in ranking_diversity:\n",
    "    cluster = df[df['ID'] == tweet.id]['cluster']\n",
    "    ranking_clusters.add(cluster.values[0])\n",
    "print('Clusters in diversified ranking:', ', '.join(map(str,ranking_clusters)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
