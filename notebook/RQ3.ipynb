{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import implicit\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.metrics import ndcg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load /home/ivan/Documents/upf/inforetrieval/finalproject/information_retrieval_final_project/search-engine/tweet.py\n",
    "import json\n",
    "\n",
    "class Tweet:\n",
    "    def __init__(self, id, text, user, date, url,\n",
    "                 hashtags, likes, retweets, terms):\n",
    "        self.id = int(id)\n",
    "        self.text = text\n",
    "        self.user = int(user)\n",
    "        self.date = date\n",
    "        self.url = url\n",
    "        self.hashtags = hashtags\n",
    "        self.likes = likes\n",
    "        self.retweets = retweets\n",
    "        self.terms = terms\n",
    "\n",
    "    def get_terms(self):\n",
    "        return self.terms\n",
    "\n",
    "    @staticmethod\n",
    "    def from_json(json_str):\n",
    "        tweet_json = json.loads(json_str)\n",
    "        return Tweet(\n",
    "            tweet_json['ID'], tweet_json['Tweet_text'], tweet_json['UserId'],\n",
    "            tweet_json['Date'], tweet_json['URL'], tweet_json['Hashtags'],\n",
    "            tweet_json['Likes'], tweet_json['Number_Retweets'], tweet_json['terms']\n",
    "        )\n",
    "\n",
    "    def to_json(self):\n",
    "        return {\n",
    "            'ID': self.id,\n",
    "            'Tweet_text': self.text,\n",
    "            'UserId': self.user,\n",
    "            'Date': self.date,\n",
    "            'URL': self.url,\n",
    "            'Hashtags': self.hashtags,\n",
    "            'Likes': self.likes,\n",
    "            'Number_Retweets': self.retweets,\n",
    "            'terms': self.terms\n",
    "        }\n",
    "\n",
    "    def tf_idf_score(self, index):\n",
    "        tweet_v = []\n",
    "        tweet_tf = index.get_tf(self, self.id)\n",
    "        idfs = index.get_idf(self)\n",
    "        for i in range(len(query_tf)):\n",
    "            tweet_v.append(tweet_tf[i] * idfs[i])\n",
    "            query_v.append(query_tf[i] * idfs[i])\n",
    "\n",
    "    def __str__(self):\n",
    "        return ('%s\\n %s | %s | %s | %s | %s | %s' %\n",
    "                (self.text, self.user, self.date, self.hashtags, self.likes, self.retweets, self.url))\n",
    "\n",
    "    def __hash__(self):\n",
    "        return self.id # we assume ids are properly assigned to unique tweets\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if not isinstance(other, Tweet):\n",
    "            return False\n",
    "        return self.id == other.id # we assume ids are properly assigned to unique tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load /home/ivan/Documents/upf/inforetrieval/finalproject/information_retrieval_final_project/search-engine/index.py\n",
    "#import scrapping.py\n",
    "import math\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pickle\n",
    "from heapq import heappush, heappushpop\n",
    "from array import array\n",
    "\n",
    "class Index:\n",
    "\n",
    "    def __init__(self, repr_size=128):\n",
    "        self.index= None # term --> tweet\n",
    "        self.tf = None\n",
    "        self.df = None\n",
    "        self.tweets = {}\n",
    "        self.max_fav = 0\n",
    "        self.max_rt = 0\n",
    "        self.repr_size = repr_size\n",
    "        self.topidf = None\n",
    "\n",
    "    def load_json_tweets(self, file):\n",
    "        with open(file, 'r') as fp:\n",
    "            self.create_index(fp.readlines())\n",
    "\n",
    "    def save(self, path):\n",
    "        with open(path, 'wb') as fp:\n",
    "            pickle.dump(self, fp)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path):\n",
    "        with open(path, 'rb') as fp:\n",
    "            index = pickle.load(fp)\n",
    "        return index\n",
    "\n",
    "    def create_index(self, lines):\n",
    "        \"\"\"\n",
    "        Implement the inverted index\n",
    "        \n",
    "        Argument:\n",
    "        lines -- collection of tweets\n",
    "        \n",
    "        Returns:\n",
    "        index - the inverted index (implemented through a python dictionary) containing terms as keys and the corresponding \n",
    "        list of tweets these keys appears in (and the positions) as values.\n",
    "        \"\"\"\n",
    "\n",
    "        index=defaultdict(list)\n",
    "        tf = defaultdict(dict)\n",
    "        df = defaultdict(int)\n",
    "\n",
    "        for line in lines: # Remember, lines contain all tweets, each line is a tweet\n",
    "            tweet_json = json.loads(line)\n",
    "            tweet = Tweet(\n",
    "                int(tweet_json['ID']), tweet_json['Tweet_text'], tweet_json['UserId'],\n",
    "                tweet_json['Date'], tweet_json['URL'], tweet_json['Hashtags'],\n",
    "                tweet_json['Likes'], tweet_json['Number_Retweets'], tweet_json['terms']\n",
    "            )\n",
    "            if tweet.id in self.tweets:\n",
    "                continue\n",
    "            self.tweets[tweet.id] = tweet\n",
    "            tweet_id = tweet.id #tweet id\n",
    "            terms = tweet.terms #page_title + page_text\n",
    "            self.max_fav = max(self.max_fav, tweet.likes)\n",
    "            self.max_rt = max(self.max_rt, tweet.retweets)\n",
    "            #title = line_arr[1]            \n",
    "            #titleIndex[page_id]=title  ## we do not need to apply get terms to title because it used only to print titles and not in the index\n",
    "            \n",
    "            ## ===============================================================        \n",
    "            ## create the index for the current doc and store it in termdictPage\n",
    "            ## termdictPage ==> { ‘term1’: [currentdoc, [list of positions]], ...,‘termn’: [currentdoc, [list of positions]]}\n",
    "            \n",
    "            ## Example: if the curr_doc has id 1 and his text is \n",
    "            ## \"web retrieval information retrieval\":\n",
    "            \n",
    "            ## termdictPage ==> { ‘web’: [1, [0]], ‘retrieval’: [1, [1,3]], ‘information’: [1, [2]]}\n",
    "            \n",
    "            ## the term ‘web’ appears in document 1 in positions 0, \n",
    "            ## the term ‘retrieval’ appears in document 1 in positions 1 and 3\n",
    "            ## ===============================================================\n",
    "            \n",
    "            termdictTweets={}\n",
    "\n",
    "            for position, term in enumerate(terms): # terms contains page_title + page_text. Loop over all terms\n",
    "                try:\n",
    "                    # if the term is already in the index for the current page (termdictPage)\n",
    "                    # append the position to the corrisponding list\n",
    "                    \n",
    "            ## START CODE\n",
    "                    termdictTweets[term][1].append(position)  \n",
    "                except:\n",
    "                    # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                    termdictTweets[term]=[tweet_id, array('I',[position])] #'I' indicates unsigned int (int in python)\n",
    "                \n",
    "            #merge the current page index with the main index\n",
    "            for termpage, postingpage in termdictTweets.items():\n",
    "                index[termpage].append(postingpage)\n",
    "\n",
    "            # normalize term frequencies\n",
    "            # Compute the denominator to normalize term frequencies (formula 2 above)\n",
    "            # norm is the same for all terms of a document.\n",
    "            norm = 0\n",
    "            for term, posting in termdictTweets.items():\n",
    "                # posting is a list containing doc_id and the list of positions for current term in current document:\n",
    "                # posting ==> [currentdoc, [list of positions]]\n",
    "                # you can use it to inferr the frequency of current term.\n",
    "                norm += len(posting[1]) ** 2\n",
    "            norm = math.sqrt(norm)\n",
    "\n",
    "            # calculate the tf(dividing the term frequency by the above computed norm) and df weights\n",
    "            for term, posting in termdictTweets.items():\n",
    "                # append the tf for current term (tf = term frequency in current doc/norm)\n",
    "                tf[term][posting[0]] = np.round(len(posting[1]) / norm, 4) ## SEE formula (1) above\n",
    "                # increment the document frequency of current term (number of documents containing the current term)\n",
    "                df[term] += 1  # increment df for current term\n",
    "            ## END CODE                    \n",
    "        self.index = index\n",
    "        self.tf = tf\n",
    "        self.df = df\n",
    "        N = len(self.tweets)\n",
    "        self.topidf = []\n",
    "        for term, df in self.df.items():\n",
    "            if len(self.topidf) < self.repr_size:\n",
    "                heappush(self.topidf, (df, term))\n",
    "            else:\n",
    "                heappushpop(self.topidf, (df, term))\n",
    "\n",
    "    #Given a query (set of words) and and index return the idf of that query\n",
    "    def get_idf(self, query):\n",
    "        N=len(self.tweets)\n",
    "        idf=[]\n",
    "        query=query.get_terms()\n",
    "        for term in query:\n",
    "            df=self.df[term]\n",
    "            idf.append(math.log(N/df))\n",
    "        return idf\n",
    "\n",
    "    #Given a query (set of words) and and index return the total tf of that query \n",
    "    def get_tf(self, query, tweet_id):\n",
    "        query=query.get_terms()\n",
    "        global_tf_list=[]\n",
    "        term_list=[]\n",
    "        for term in query:\n",
    "            term_list.append(self.tf[term][tweet_id])\n",
    "        return term_list\n",
    "\n",
    "    def get_tf_idf_vector(self, tweet_id):\n",
    "        vector = np.zeros(self.repr_size)\n",
    "        N = len(self.tweets)\n",
    "        total = 0\n",
    "        for i, (df, term) in enumerate(self.topidf):\n",
    "            tf = self.tf[term].get(tweet_id, 0)\n",
    "            tf_idf = tf * math.log(N/df)\n",
    "            vector[i] = tf_idf\n",
    "        norm = np.linalg.norm(vector)\n",
    "        if norm == 0:\n",
    "            return vector\n",
    "        return vector / norm\n",
    "\n",
    "    def get_all_tf_idf(self):\n",
    "        vectors = np.array([])\n",
    "        for tweet in self.tweets:\n",
    "            v = self.get_tf_idf_vector(tweet)\n",
    "            if isinstance(v, np.ndarray):\n",
    "                vectors = np.vstack((vectors, v))\n",
    "            if len(vectors) % 1000 == 0:\n",
    "                print(len(vectors))\n",
    "        return vectors\n",
    "\n",
    "    #Given a query (set of words) return all the tweets containing the query\n",
    "    def get_tweets(self, query):\n",
    "        query=query.get_terms()\n",
    "        list_tweets=set()\n",
    "        for term in query:\n",
    "            ## START DODE\n",
    "            try:\n",
    "                # store in termDocs the ids of the docs that contain \"term\"\n",
    "                termTweets=[posting[0] for posting in self.index[term]]\n",
    "                # docs = docs Union termDocs\n",
    "                if len(list_tweets) == 0:\n",
    "                    list_tweets = list_tweets.union(termTweets)\n",
    "                else:\n",
    "                    list_tweets = list_tweets.intersection(termTweets)\n",
    "            except:\n",
    "                #term is not in index\n",
    "                pass\n",
    "        list_tweets=list(list_tweets)\n",
    "\n",
    "        tweets_query = [self.tweets[tid] for tid in list_tweets] #asssuming index of tweets is id\n",
    "        #list of tweets ids\n",
    "        return tweets_query\n",
    "\n",
    "    #Given a query (set of words) return the tweets number of tweets containing the query\n",
    "    def get_num_tweets(self,query):\n",
    "\n",
    "        num_tweets=len(self.get_tweets(query))\n",
    "\n",
    "        return num_tweets\n",
    "\n",
    "    def get_max_fav(self):\n",
    "        return self.max_fav\n",
    "\n",
    "    def get_max_rt(self):\n",
    "        return self.max_rt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b, norm_a=True, norm_b=True):\n",
    "    total = 0\n",
    "    sum_a = 0 if norm_a else 1.0\n",
    "    sum_b = 0 if norm_b else 1.0\n",
    "    for x, y in zip(a, b):\n",
    "        total += x * y\n",
    "        if norm_a:\n",
    "            sum_a += x * x\n",
    "        if norm_b:\n",
    "            sum_b += y * y\n",
    "    if norm_a:\n",
    "        sum_a = math.sqrt(sum_a)\n",
    "    if norm_b:\n",
    "        sum_b = math.sqrt(sum_b)\n",
    "    return total / (sum_a * sum_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load /home/ivan/Documents/upf/inforetrieval/finalproject/information_retrieval_final_project/search-engine/user.py\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "class User:\n",
    "    def __init__(self, id, username, mentions):\n",
    "        self.id = int(id)\n",
    "        self.username = username\n",
    "        self.mentions = mentions\n",
    "\n",
    "    def add_mention(self, uid):\n",
    "        if uid not in self.mentions:\n",
    "            self.mentions[uid] = 0\n",
    "        self.mentions[uid] += 1\n",
    "\n",
    "    @staticmethod\n",
    "    def from_dict(data):\n",
    "        mentions = {int(user): int(mention) for user, mention in data['MENTIONS'].items()}\n",
    "        return User(int(data['USERID']), data['USERNAME'], mentions)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_json(json_str):\n",
    "        user_data = json.loads(json_str)\n",
    "        return User.from_dict(user_data)\n",
    "\n",
    "    def to_json(self):\n",
    "        return {\n",
    "            'USERID' : self.id,\n",
    "            'USERNAME' : self.username,\n",
    "            'MENTIONS' : self.mentions}\n",
    "\n",
    "class UserTweets:\n",
    "    def __init__(self, user, tweets):\n",
    "        self.user = user\n",
    "        self.tweets = tweets\n",
    "\n",
    "    def add_tweet(self, tweet):\n",
    "        self.tweets.append(tweet)\n",
    "\n",
    "    def get_vector(self, index):\n",
    "        return np.mean([index.get_tf_idf_vector(t.id) for t in self.tweets], axis=0)\n",
    "\n",
    "def load_users(file):\n",
    "    with open(str(file), 'r') as fp:\n",
    "        for line in fp.readlines():\n",
    "            file_users = json.loads(line)\n",
    "    return {int(uid):User.from_dict(user_dict)\n",
    "                for uid, user_dict in file_users.items()}\n",
    "\n",
    "def get_user_tweets(users, index):\n",
    "    user_tweets = {}\n",
    "    for tweet in index.tweets.values():\n",
    "        if tweet.user in users:\n",
    "            user = users[tweet.user]\n",
    "            if user.id not in user_tweets:\n",
    "                user_tweets[user.id] = UserTweets(user, [])\n",
    "            user_tweets[user.id].add_tweet(tweet)\n",
    "    return user_tweets\n",
    "\n",
    "def user_relevance(user1: UserTweets, user2: UserTweets):\n",
    "    return cosine_similarity(user1.get_vector(), user2.get_vector())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_user_graph(users):\n",
    "    graph = nx.Graph()\n",
    "    for user in users.values():\n",
    "        graph.add_node(user.id)\n",
    "    for user in users.values():\n",
    "        for mention in user.mentions.keys():\n",
    "            graph.add_edge(user.id, mention)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = load_users('../res/merge_users.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = create_user_graph(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = Index()\n",
    "index.load_json_tweets('../res/merge_tweets_wusers.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 161469\n",
      "Number of edges: 114622\n"
     ]
    }
   ],
   "source": [
    "print('Number of nodes:', len(graph.nodes()))\n",
    "print('Number of edges:', len(graph.edges()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_graph(graph, p_test):\n",
    "    E_test = int(len(graph.edges) * p_test)\n",
    "    test_edge_list_split = random.sample(graph.edges(), E_test)\n",
    "    test_edge_list = list(test_edge_list_split)\n",
    "    # Remove some edges\n",
    "    training_graph = graph.copy()\n",
    "    training_graph.remove_edges_from(test_edge_list_split)\n",
    "    test_graph = nx.Graph()\n",
    "    test_graph.add_edges_from(test_edge_list)\n",
    "    return training_graph, test_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_graph(graph, max_nodes):\n",
    "    if len(graph.nodes()) <= max_nodes:\n",
    "        return graph\n",
    "    sample_nodes = random.sample(graph.nodes(), max_nodes)\n",
    "    g = nx.Graph()\n",
    "    for node in sample_nodes:\n",
    "        g.add_node(node)\n",
    "    for u, v in graph.edges():\n",
    "        if u in g.nodes() and v in g.nodes():\n",
    "            g.add_edge(u, v)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_graph = sample_graph(graph, 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_tweets = get_user_tweets(users, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 50000\n",
      "Number of edges: 13427\n"
     ]
    }
   ],
   "source": [
    "print('Number of nodes:', len(small_graph.nodes()))\n",
    "print('Number of edges:', len(small_graph.edges()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = split_graph(small_graph, 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes in train: 50000\n",
      "Number of nodes in test: 3397\n",
      "Number of edges in train: 10742\n",
      "Number of edges in test: 2685\n"
     ]
    }
   ],
   "source": [
    "print('Number of nodes in train:', len(train.nodes()))\n",
    "print('Number of nodes in test:', len(test.nodes()))\n",
    "print('Number of edges in train:', len(train.edges()))\n",
    "print('Number of edges in test:', len(test.edges()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def friends_of_friends(start, graph):\n",
    "    friends1 = set(graph.neighbors(start))\n",
    "    friends = set()\n",
    "    for friend in friends1:\n",
    "        friends = friends.union(set(graph.neighbors(friend)))\n",
    "    friends = friends.difference({start})\n",
    "    friends = friends.difference(friends1)\n",
    "    return friends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MichaelTheStdnt'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users[2596370295].username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jacobinmag\n",
      "shantilly_t\n",
      "ASNinaWrites\n",
      "rabbitven0m\n",
      "AntifaVio\n",
      "TeenVogue\n",
      "prolpo\n",
      "cit_uprising\n",
      "JamaalBowmanNY\n",
      "knjatz\n",
      "SmythLr\n",
      "yimmygee\n",
      "RaceJustice\n",
      "reinedeloup\n",
      "MrErnestOwens\n",
      "shannondrewthis\n",
      "QasimRashid\n"
     ]
    }
   ],
   "source": [
    "for user in friends_of_friends(2596370295, small_graph):\n",
    "    print(users[user].username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg_graph(recommendations, graph, K=10):\n",
    "    scores = []\n",
    "    true_rel = []\n",
    "    for user in recommendations:\n",
    "        score = [sc for u, sc in recommendations[user][:K]]\n",
    "        if len(score) == 0:\n",
    "            continue\n",
    "        if len(score) < K:\n",
    "            score += [0] * (K - len(score))\n",
    "        t_rel = [int((user, u) in graph.edges()) for u, sc in recommendations[user][:K]]\n",
    "        if len(t_rel) < K:\n",
    "            t_rel += [0] * (K - len(t_rel))\n",
    "        scores.append(score)\n",
    "        true_rel.append(t_rel)\n",
    "    return ndcg_score(np.array(true_rel), np.array(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank(g_train, g_test, graph, K=10):\n",
    "    recommendations = {}\n",
    "    N_test = len(g_test.nodes())\n",
    "    pr = nx.pagerank(g_train,personalization={node: 1/N_test for node in g_test.nodes()})\n",
    "    recommendations = {}\n",
    "    for node in dict(pr).keys():\n",
    "        friends = friends_of_friends(node, graph)\n",
    "        node_scores = [(n, pr[node]) for n in friends]\n",
    "        node_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        recommendations[node] = node_scores[:K]\n",
    "        recommendations[node] = [(u, score/recommendations[node][0][1]) for u, score in recommendations[node]]\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "pager = pagerank(train, test, small_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jacobinmag\n",
      "shantilly_t\n",
      "ASNinaWrites\n",
      "rabbitven0m\n",
      "AntifaVio\n",
      "TeenVogue\n",
      "prolpo\n",
      "cit_uprising\n",
      "JamaalBowmanNY\n",
      "knjatz\n"
     ]
    }
   ],
   "source": [
    "for user, _ in pager[2596370295]:\n",
    "    print(users[user].username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adamic_adar(g_train, g_test, graph, K=10):\n",
    "    edges = set()\n",
    "    for node in g_test.nodes():\n",
    "        edges = edges.union({(node, friend) for friend in friends_of_friends(node, graph)})\n",
    "    preds = nx.adamic_adar_index(g_train, list(edges))\n",
    "    recommendation = {}\n",
    "    for u, v, p in preds:\n",
    "        if u not in recommendation:\n",
    "            recommendation[u] = []\n",
    "        recommendation[u].append((v, p))\n",
    "    for u, rank in recommendation.items():\n",
    "        rank.sort(key=lambda x: x[1], reverse=True)\n",
    "        recommendation[u] = rank[:K]\n",
    "    return recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "adamic_result = adamic_adar(train, test, small_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rabbitven0m\n",
      "reinedeloup\n",
      "JamaalBowmanNY\n",
      "RaceJustice\n",
      "cit_uprising\n",
      "prolpo\n",
      "MrErnestOwens\n",
      "yimmygee\n",
      "shannondrewthis\n",
      "knjatz\n"
     ]
    }
   ],
   "source": [
    "for user, _ in adamic_result[2596370295]:\n",
    "    print(users[user].username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def als(g_train, g_test, graph, K=10):\n",
    "    als = implicit.als.AlternatingLeastSquares()\n",
    "    als.fit(nx.adjacency_matrix(g_train, nodelist=graph.nodes()))\n",
    "    index = {node:i for i, node in enumerate(graph.nodes())}\n",
    "    A = nx.adjacency_matrix(graph)\n",
    "    recommendations = {}\n",
    "    for node in g_test.nodes():\n",
    "        ff = [index[friend] for friend in friends_of_friends(node, graph)]\n",
    "        if len(ff) == 0:\n",
    "            recommendations[node] = []\n",
    "            continue\n",
    "        rank = als.rank_items(index[node], A, ff)\n",
    "        recommendations[node] = [(list(graph.nodes())[i], score) for i, score in rank[:K]]\n",
    "    return recommendations\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a294836b03c4ff0bc88334f57287f6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "als_result = als(train, test, small_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rabbitven0m\n",
      "SmythLr\n",
      "shantilly_t\n",
      "RaceJustice\n",
      "shannondrewthis\n",
      "MrErnestOwens\n",
      "reinedeloup\n",
      "prolpo\n",
      "yimmygee\n",
      "knjatz\n"
     ]
    }
   ],
   "source": [
    "for user, _ in als_result[2596370295]:\n",
    "    print(users[user].username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_relevance(g_train, g_test, graph, user_tweets, index, K=10):\n",
    "    recommendations = {}\n",
    "    for user in g_test.nodes():\n",
    "        recommendations[user] = []\n",
    "        if user not in user_tweets:\n",
    "            continue\n",
    "        vector = user_tweets[user].get_vector(index)\n",
    "        for friend in friends_of_friends(user, g_train):\n",
    "            if friend in user_tweets:\n",
    "                friend_vector = user_tweets[friend].get_vector(index)\n",
    "                recommendations[user].append((friend, cosine_similarity(vector, friend_vector)))\n",
    "        recommendations[user].sort(key=lambda x: x[1], reverse=True)\n",
    "        recommendations[user] = recommendations[user][:K]\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivan/.local/lib/python3.6/site-packages/ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "custom = custom_relevance(train, test, small_graph, user_tweets, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reinedeloup\n",
      "ASNinaWrites\n",
      "jacobinmag\n",
      "TeenVogue\n",
      "JamaalBowmanNY\n",
      "prolpo\n",
      "yimmygee\n",
      "rabbitven0m\n",
      "MrErnestOwens\n",
      "AntifaVio\n"
     ]
    }
   ],
   "source": [
    "for user, _ in custom[2596370295]:\n",
    "    print(users[user].username)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
